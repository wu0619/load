{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 10:58:39.481086: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-04 10:58:39.499859: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-04 10:58:39.499879: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-04 10:58:39.499891: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-04 10:58:39.503729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from keras.activations import sigmoid\n",
    "from keras.models import Model ,load_model\n",
    "from keras.layers import Input, Dense, ConvLSTM2D, Conv2D, Conv1D, MaxPooling2D, Layer, GlobalAveragePooling2D, Reshape, Flatten, BatchNormalization\n",
    "from keras.regularizers import L2\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.saving import register_keras_serializable\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.sparse.linalg import cg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 10:58:40.661869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.661966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.664548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.664653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.664719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.664781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.665780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.665869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.665931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.699816: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.699915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.699985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-04 10:58:40.700059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9278 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs\n",
    "DATA_DIR = \"./load.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_DIR)\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%Y/%m/%d %H:%M')\n",
    "data['Load'] = data['Load'] * 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data['Load'].to_numpy().reshape(-1, 1))\n",
    "data['Load'] = data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe dataset columns must be the format below:\\n\\n  index             Timestamp   Load\\n      0   20xx-xx-xx xx:xx:xx    xxx\\n      1                   ...    ...\\n      2                   ...    ...\\n                                 ...\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The dataset columns must be the format below:\n",
    "\n",
    "  index             Timestamp   Load\n",
    "      0   20xx-xx-xx xx:xx:xx    xxx\n",
    "      1                   ...    ...\n",
    "      2                   ...    ...\n",
    "                                 ...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>0.445492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:15:00</td>\n",
       "      <td>0.427049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:30:00</td>\n",
       "      <td>0.445492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:45:00</td>\n",
       "      <td>0.420902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>0.422951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35035</th>\n",
       "      <td>2023-12-31 22:45:00</td>\n",
       "      <td>0.331148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35036</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>0.270492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35037</th>\n",
       "      <td>2023-12-31 23:15:00</td>\n",
       "      <td>0.365574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35038</th>\n",
       "      <td>2023-12-31 23:30:00</td>\n",
       "      <td>0.337295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35039</th>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "      <td>0.253689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35040 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp      Load\n",
       "0     2023-01-01 00:00:00  0.445492\n",
       "1     2023-01-01 00:15:00  0.427049\n",
       "2     2023-01-01 00:30:00  0.445492\n",
       "3     2023-01-01 00:45:00  0.420902\n",
       "4     2023-01-01 01:00:00  0.422951\n",
       "...                   ...       ...\n",
       "35035 2023-12-31 22:45:00  0.331148\n",
       "35036 2023-12-31 23:00:00  0.270492\n",
       "35037 2023-12-31 23:15:00  0.365574\n",
       "35038 2023-12-31 23:30:00  0.337295\n",
       "35039 2023-12-31 23:45:00  0.253689\n",
       "\n",
       "[35040 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!! parameter settings\n",
    "n_predict: predict steps\n",
    "height: final height of the image:\n",
    "            height * 2 if the n_predict <= width,\n",
    "            height * 2 + 1 if the n_predict > width\n",
    "width: width of the image\n",
    "n_days: use past n days historical time series data as input (number of channel)\n",
    "n_window_shift: the shift interval of sliding window\n",
    "\"\"\"\n",
    "n_predict = 12\n",
    "height = 4\n",
    "width = 24\n",
    "n_days_b = 3\n",
    "n_days_s = 3\n",
    "n_window_shift = \"15min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesImageEncoder():\n",
    "    def __init__(\n",
    "            self,\n",
    "        X: pd.DataFrame,\n",
    "        n_predict: int,\n",
    "        height: int,\n",
    "        width: int,\n",
    "        n_days_b: int,\n",
    "        n_days_s: int,\n",
    "        n_window_shift: str\n",
    "    ) -> None:\n",
    "        self.X = X\n",
    "        self.h = height\n",
    "        self.m = width\n",
    "        self.d_b = n_days_b\n",
    "        self.d_s = n_days_s\n",
    "        self.shift = n_window_shift\n",
    "        self.n_predict = n_predict\n",
    "        self.Lb = self.h * self.m - self.n_predict\n",
    "        self.Ls = math.ceil(self.n_predict / self.m) * self.m - self.n_predict\n",
    "        self.timestamps = self.generate_timestamps()\n",
    "        print(f\"Lb: {self.Lb}\")\n",
    "        print(f\"Ls: {self.Ls}\")\n",
    "\n",
    "    def generate_timestamps(self):\n",
    "        start = self.X['Timestamp'].min() + DateOffset(days=self.d_b)\n",
    "        end = self.X['Timestamp'].max() - DateOffset(minutes=self.n_predict*15)\n",
    "        timestamps = pd.date_range(start=start, end=end, freq=self.shift)\n",
    "        return timestamps\n",
    "    \n",
    "    def generate_gaussian_noise(self, length, std_dev=0.15):\n",
    "        noise = np.random.normal(loc=0.5, scale=std_dev, size=length)\n",
    "        noise = np.clip(noise, 0, 1)\n",
    "        # noise = np.zeros(shape=length)\n",
    "        return pd.DataFrame({\"Load\": noise})\n",
    "    \n",
    "    def make_it_symmetric_3d(self, sets_3d):\n",
    "        symmetry_training_sets = []\n",
    "        for slice_2d in np.array(sets_3d):\n",
    "            reversed_slice_2d = slice_2d[::-1]\n",
    "            combined_slice_2d = np.concatenate((slice_2d, reversed_slice_2d), axis=0)\n",
    "            symmetry_training_sets.append(combined_slice_2d)\n",
    "        return np.array(symmetry_training_sets)\n",
    "    \n",
    "    def make_it_symmetric_2d(self, sets_2d):\n",
    "        combined_slice = np.concatenate((sets_2d, sets_2d), axis=0)\n",
    "        return np.array(combined_slice).reshape(2, int(len(combined_slice)/2))\n",
    "    \n",
    "\n",
    "    def encode_b(self):\n",
    "        training_sets = []\n",
    "        target_sets = []\n",
    "        self.X_timeseries_flatten = []\n",
    "        self.X_timestamp = []\n",
    "        self.y_timestamp = []\n",
    "        for steps in self.timestamps:\n",
    "            training_start_b = steps - DateOffset(days=self.d_b-1, hours=23, minutes=45)\n",
    "            training_end = steps\n",
    "            target_start = training_end + DateOffset(minutes=15)\n",
    "            target_end = steps + DateOffset(minutes=(self.n_predict)*15)\n",
    "            # noise = self.generate_gaussian_noise(length=self.n_predict)\n",
    "            training_data = self.X[(self.X['Timestamp'] >= training_start_b) & (self.X['Timestamp'] <= training_end)]\n",
    "            # training_data = pd.concat([training_data, noise], ignore_index=True)\n",
    "            target_data = self.X[(self.X['Timestamp'] >= target_start) & (self.X['Timestamp'] <= target_end)]\n",
    "            if not training_data.empty and not target_data.empty:\n",
    "                self.X_timeseries_flatten.append(training_data['Load'])\n",
    "                self.X_timestamp.append(training_data['Timestamp'])\n",
    "                self.y_timestamp.append(target_data['Timestamp'])\n",
    "                training_reshaped = np.array(training_data['Load']).reshape(self.d_b, self.h, self.m)\n",
    "                symmetric_3d = self.make_it_symmetric_3d(training_reshaped)\n",
    "                training_sets.append(symmetric_3d)\n",
    "                symmetric_2d = self.make_it_symmetric_2d(target_data['Load'])\n",
    "                target_sets.append(symmetric_2d)\n",
    "        training_sets = np.array(training_sets)\n",
    "        target_sets = np.array(target_sets)\n",
    "\n",
    "        self.X_timeseries_flatten = np.array(self.X_timeseries_flatten)\n",
    "        self.X_timestamp = np.array(self.X_timestamp)\n",
    "        self.y_timestamp = np.array(self.y_timestamp)\n",
    "        return training_sets, target_sets\n",
    "    \n",
    "    def encode_s(self):\n",
    "        training_sets = []\n",
    "        for steps in self.timestamps:\n",
    "            training_subset = []\n",
    "            point = steps - DateOffset(days=self.d_s-1)\n",
    "            training_start = point - DateOffset(minutes=(self.m-1)*15)\n",
    "            # training\n",
    "            for _ in range(self.d_s-1):\n",
    "                training_end = training_start + DateOffset(minutes=(self.m-1)*15)\n",
    "                training_data = self.X[(self.X['Timestamp'] >= training_start) & (self.X['Timestamp'] <= training_end)]\n",
    "                if not training_data.empty:\n",
    "                    symmetric_2d = self.make_it_symmetric_2d(training_data['Load'])\n",
    "                    training_subset.append(symmetric_2d)\n",
    "                training_start = training_start + DateOffset(days=1)\n",
    "            training_end = training_start + DateOffset(minutes=(self.m-self.n_predict-1)*15)\n",
    "            training_data = self.X[(self.X['Timestamp'] >= training_start) & (self.X['Timestamp'] <= training_end)]\n",
    "            noise = self.generate_gaussian_noise(length=self.n_predict)\n",
    "            training_data = pd.concat([training_data, noise], ignore_index=True)\n",
    "            symmetric_2d = self.make_it_symmetric_2d(training_data['Load'])\n",
    "            training_subset.append(symmetric_2d)\n",
    "            training_sets.append(training_subset)\n",
    "        training_sets = np.array(training_sets)\n",
    "        return training_sets\n",
    "    \n",
    "    def encode(self):\n",
    "        training_sets_b, target_sets = self.encode_b()\n",
    "        # training_sets_s = self.encode_s()\n",
    "        training_sets_b = np.transpose(training_sets_b, (0, 2, 3, 1))\n",
    "        # training_sets_s = np.transpose(training_sets_s, (0, 2, 3, 1))\n",
    "        return training_sets_b, target_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lb: 84\n",
      "Ls: 12\n"
     ]
    }
   ],
   "source": [
    "encoder = TimeSeriesImageEncoder(\n",
    "    X=data,\n",
    "    n_predict=n_predict,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    n_days_b=n_days_b,\n",
    "    n_days_s=n_days_s,\n",
    "    n_window_shift=n_window_shift\n",
    ")\n",
    "encoded_Xb, encoded_y = encoder.encode()\n",
    "X_timeseries = np.copy(encoder.X_timeseries_flatten)\n",
    "X_timestamp = np.copy(encoder.X_timestamp)\n",
    "y_timestamp = np.copy(encoder.y_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34740, 8, 24, 3)\n",
      "(34740, 2, 12)\n",
      "(34740, 288)\n",
      "(34740, 288)\n",
      "(34740, 12)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_Xb.shape)\n",
    "# print(encoded_Xs.shape)\n",
    "print(encoded_y.shape)\n",
    "\n",
    "print(X_timeseries.shape)\n",
    "print(X_timestamp.shape)\n",
    "print(y_timestamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_TIME_STEP = math.floor(encoder.timestamps.shape[0] / 24)\n",
    "X_test_b = []\n",
    "y_test = []\n",
    "X_test_b_flatten = []\n",
    "X_test_b_timestamp = []\n",
    "y_test_timestamp = []\n",
    "\n",
    "for i in range(0, 24):\n",
    "    start = (i+1)*MONTH_TIME_STEP-(192*(i+1))\n",
    "    end = (i+1)*MONTH_TIME_STEP-(192*i)\n",
    "    X_test_b.append(encoded_Xb[start:end])\n",
    "    y_test.append(encoded_y[start:end])\n",
    "    X_test_b_flatten.append(X_timeseries[start:end])\n",
    "    X_test_b_timestamp.append(X_timestamp[start:end])\n",
    "    y_test_timestamp.append(y_timestamp[start:end])\n",
    "\n",
    "\n",
    "    encoded_Xb = np.concatenate([encoded_Xb[:start], encoded_Xb[end:]])\n",
    "    encoded_y = np.concatenate([encoded_y[:start], encoded_y[end:]])\n",
    "    X_timeseries = np.concatenate([X_timeseries[:start], X_timeseries[end:]])\n",
    "    X_timestamp = np.concatenate([X_timestamp[:start], X_timestamp[end:]])\n",
    "    y_timestamp = np.concatenate([y_timestamp[:start], y_timestamp[end:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_b = np.concatenate([i for i in X_test_b])\n",
    "y_test = np.concatenate([i for i in y_test])\n",
    "X_test_b_flatten = np.concatenate([i for i in X_test_b_flatten])\n",
    "X_test_b_timestamp = np.concatenate([i for i in X_test_b_timestamp])\n",
    "y_test_timestamp = np.concatenate([i for i in y_test_timestamp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b = encoded_Xb\n",
    "y_train = encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30132, 8, 24, 3)\n",
      "(4608, 8, 24, 3)\n",
      "(30132, 2, 12)\n",
      "(4608, 2, 12)\n",
      "(4608, 288)\n",
      "(4608, 288)\n",
      "(4608, 12)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(X_train_b).shape)\n",
    "print(np.array(X_test_b).shape)\n",
    "print(np.array(y_train).shape)\n",
    "print(np.array(y_test).shape)\n",
    "print(X_test_b_flatten.shape)\n",
    "print(X_test_b_timestamp.shape)\n",
    "print(y_test_timestamp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable('ECALayer')\n",
    "class ECALayer(Layer):\n",
    "    def __init__(self, gamma=2, b=1, **kwargs):\n",
    "        super(ECALayer, self).__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.b = b\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        c = input_shape[-1]\n",
    "        self.t = max(1, int(abs((tf.math.log(float(c)) / tf.math.log(2.0) + self.b) / self.gamma)))\n",
    "        self.conv = Conv1D(filters=1, kernel_size=self.t, padding='same', use_bias=False)\n",
    "        super(ECALayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Global Average Pooling over the spatial dimensions to produce a (batch_size, 1, channels) tensor\n",
    "        x = GlobalAveragePooling2D()(inputs)\n",
    "        x = Reshape((1, -1))(x)\n",
    "        x = self.conv(x)\n",
    "        x = sigmoid(x)\n",
    "        x = tf.squeeze(x, axis=1)  # Squeeze to make it (batch_size, channels)\n",
    "        \n",
    "        # Multiply weights across channels\n",
    "        return inputs * x[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ECALayer, self).get_config()\n",
    "        config.update({\n",
    "            'gamma': self.gamma,\n",
    "            'b': self.b\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable('AverageLayer')\n",
    "class AverageLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AverageLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (inputs[0] + inputs[1]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(input_shape_b, input_shape_s, num_outputs):\n",
    "#     inputs_b = Input(shape=input_shape_b)\n",
    "#     inputs_s = Input(shape=input_shape_s)\n",
    "#     conv1 = Conv2D(filters=32, kernel_size=8, padding=\"same\", activation=\"tanh\")(inputs_b)\n",
    "#     conv2 = Conv2D(filters=64, kernel_size=8, padding=\"same\", activation=\"tanh\")(conv1)\n",
    "#     conv2 = Reshape((1, *conv2.shape[1:]))(conv2)  \n",
    "#     lstm1 = ConvLSTM2D(filters=96, kernel_size=8, padding=\"same\", activation=\"tanh\", return_sequences=True, dropout=0.3)(conv2)\n",
    "#     lstm2 = ConvLSTM2D(filters=96, kernel_size=8, padding=\"same\", activation=\"tanh\", return_sequences=False, dropout=0.3)(lstm1)\n",
    "#     eca1 = ECALayer()(lstm2)\n",
    "#     conv3 = Conv2D(filters=64, kernel_size=8, padding=\"same\", activation=\"tanh\")(eca1)\n",
    "#     conv4 = Conv2D(filters=32, kernel_size=8, padding=\"same\", activation=\"tanh\")(conv3)\n",
    "#     maxpool1 = MaxPooling2D(pool_size=10, padding=\"same\")(conv4)\n",
    "#     flatten1 = Flatten()(maxpool1)\n",
    "#     dense1 = Dense(2*num_outputs, activation=\"linear\")(flatten1)\n",
    "#     outputs1 = Reshape((2, 12))(dense1)\n",
    "\n",
    "#     conv5 = Conv2D(filters=8, kernel_size=2, padding=\"same\", activation=\"tanh\")(inputs_s)\n",
    "#     conv6 = Conv2D(filters=16, kernel_size=2, padding=\"same\", activation=\"tanh\")(conv5)\n",
    "#     conv6 = Reshape((1, *conv6.shape[1:]))(conv6)  \n",
    "#     lstm3 = ConvLSTM2D(filters=24, kernel_size=2, padding=\"same\", activation=\"tanh\", return_sequences=True, dropout=0.3)(conv6)\n",
    "#     lstm4 = ConvLSTM2D(filters=24, kernel_size=2, padding=\"same\", activation=\"tanh\", return_sequences=False, dropout=0.3)(lstm3)\n",
    "#     eca2 = ECALayer()(lstm4)\n",
    "#     conv7 = Conv2D(filters=16, kernel_size=2, padding=\"same\", activation=\"tanh\")(eca2)\n",
    "#     conv8 = Conv2D(filters=8, kernel_size=2, padding=\"same\", activation=\"tanh\")(conv7)\n",
    "#     maxpool2 = MaxPooling2D(pool_size=5, padding=\"same\")(conv8)\n",
    "#     flatten2 = Flatten()(maxpool2)\n",
    "#     dense2 = Dense(2*num_outputs, activation=\"linear\")(flatten2)\n",
    "#     outputs2 = Reshape((2, 12))(dense2)\n",
    "\n",
    "#     final_output = AverageLayer()([outputs1, outputs2])\n",
    "#     model = Model(inputs=[inputs_b, inputs_s], outputs=final_output)\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape_b, num_outputs):\n",
    "    inputs_b = Input(shape=input_shape_b)\n",
    "    conv1 = Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")(inputs_b)\n",
    "    conv2 = Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"tanh\")(conv1)\n",
    "    conv2 = Reshape((1, *conv2.shape[1:]))(conv2)\n",
    "    nor1 = BatchNormalization()(conv2)\n",
    "    lstm1 = ConvLSTM2D(filters=96, kernel_size=3, padding=\"same\", activation=\"tanh\", return_sequences=True, dropout=0.0)(nor1)\n",
    "    lstm2 = ConvLSTM2D(filters=96, kernel_size=3, padding=\"same\", activation=\"tanh\", return_sequences=False, dropout=0.0)(lstm1)\n",
    "    eca1 = ECALayer()(lstm2)\n",
    "    nor2 = BatchNormalization()(eca1)\n",
    "    conv3 = Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"tanh\")(nor2)\n",
    "    conv4 = Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")(conv3)\n",
    "    maxpool1 = MaxPooling2D(pool_size=10, padding=\"same\")(conv4)\n",
    "    flatten1 = Flatten()(maxpool1)\n",
    "    dense1 = Dense(2*num_outputs, activation=\"linear\")(flatten1)\n",
    "    outputs = Reshape((2, 12))(dense1)\n",
    "    model = Model(inputs=inputs_b, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8, 24, 3)]        0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 8, 24, 32)         896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 24, 64)         18496     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 8, 24, 64)      0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1, 8, 24, 64)      256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 1, 8, 24, 96)      553344    \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 8, 24, 96)         663936    \n",
      "                                                                 \n",
      " eca_layer (ECALayer)        (None, 8, 24, 96)         288       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 8, 24, 96)         384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 24, 64)         55360     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 24, 32)         18464     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 1, 3, 32)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 96)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                2328      \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 2, 12)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1313752 (5.01 MB)\n",
      "Trainable params: 1313432 (5.01 MB)\n",
      "Non-trainable params: 320 (1.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "early_stopping_callback = EarlyStopping(monitor=\"loss\", patience=10, min_delta=5e-5)\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor=\"loss\", factor=0.3, patience=5, verbose=1, min_lr=1e-7)\n",
    "callbacks=[tensorboard_callback, early_stopping_callback, reduce_lr_callback]\n",
    "model = create_model(input_shape_b=X_train_b.shape[1:], num_outputs=12)\n",
    "model.compile(optimizer=Adam(learning_rate=5e-5), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lbfgs(model, loss_fn, x_data, y_data, learning_rate_theta=1e-5):\n",
    "    # flatten model parameters theta to 1-dim array\n",
    "    initial_params = tf.concat([tf.reshape(param, [-1]) for param in model.trainable_variables], axis=0)\n",
    "\n",
    "    # define a function to calculate loss and gradient\n",
    "    def value_and_gradients_function(params):\n",
    "        # update model parameter theta\n",
    "        assign_new_model_parameters(model, params)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(model.trainable_variables)\n",
    "            predictions = model(x_data, training=True)\n",
    "            loss = loss_fn(y_data, predictions)\n",
    "        # calculate the loss gradient w.r.t model parameters theta\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        flat_grads = tf.concat([tf.reshape(grad, [-1]) for grad in grads], axis=0)\n",
    "        return loss, flat_grads\n",
    "\n",
    "    # execute L-BFGS optimization\n",
    "    results = tfp.optimizer.lbfgs_minimize(\n",
    "        value_and_gradients_function,\n",
    "        initial_position=initial_params,\n",
    "        tolerance=1e-8  # adjust to appropriate training tolerance\n",
    "    )\n",
    "\n",
    "    # assign new model parameter theta\n",
    "    assign_new_model_parameters(model, results.position)\n",
    "\n",
    "def assign_new_model_parameters(model, flat_params):\n",
    "    \"\"\" Update model parameter theta \"\"\"\n",
    "    start = 0\n",
    "    for param in model.trainable_variables:\n",
    "        size = tf.size(param)\n",
    "        new_shape = tf.shape(param)\n",
    "        param.assign(tf.reshape(flat_params[start:start + size], new_shape))\n",
    "        start += size\n",
    "\n",
    "def update_weights(model, X_val, y_val, weights, learning_rate_w):\n",
    "    \"\"\" Update the weight of the sample \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(weights)\n",
    "        predictions = model(X_val, training=True)\n",
    "        loss = tf.reduce_mean(weights * tf.keras.losses.mean_squared_error(y_val[:, 0], tf.reduce_mean(predictions, axis=1, keepdims=True)))\n",
    "    grads = tape.gradient(loss, weights)\n",
    "    new_weights = tf.clip_by_value(weights - learning_rate_w * grads, 1e-5, 1)\n",
    "    return new_weights\n",
    "\n",
    "def training(model, X_train, y_train, X_val, y_val, epochs, batch_size, callbacks):\n",
    "    for callback in callbacks:\n",
    "        callback.set_model(model)\n",
    "        callback.on_train_begin()\n",
    "        \n",
    "    weights = tf.Variable(np.ones(len(X_train)) / len(X_train), dtype=tf.float32)\n",
    "    # learning rate of the sample weight\n",
    "    learning_rate_w = 5e-5\n",
    "    # learning rate of the model parameters θ\n",
    "    learning_rate_theta = 1e-5\n",
    "    # optimizer of model parameters θ\n",
    "    optimizer_theta = Adam(learning_rate=learning_rate_theta)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        for callback in callbacks:\n",
    "            callback.on_epoch_begin(epoch)\n",
    "\n",
    "        for batch in range((len(X_train) + batch_size - 1) // batch_size):\n",
    "            start = batch * batch_size\n",
    "            end = min(start + batch_size, len(X_train))\n",
    "\n",
    "            apply_lbfgs(model, lambda y_true, y_pred: tf.reduce_mean(weights[start:end] * tf.keras.losses.mean_squared_error(y_true[:, 0], tf.reduce_mean(y_pred, axis=1, keepdims=True))), X_train[start:end], y_train[start:end])\n",
    "        \n",
    "        weights.assign(update_weights(model, X_val, y_val, weights, learning_rate_w=learning_rate_w))\n",
    "\n",
    "        predictions = model(X_train, training=True)\n",
    "        val_loss = tf.reduce_mean(weights * tf.keras.losses.mean_squared_error(y_train[:, 0], tf.reduce_mean(predictions, axis=1, keepdims=True)))\n",
    "        print(f\"val_loss: {val_loss.numpy()}\")\n",
    "        \n",
    "        if early_stopping_callback.stopped_epoch:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    for callback in callbacks:\n",
    "        callback.on_train_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 10:59:04.789841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 13s 33ms/step - loss: 0.0244 - lr: 5.0000e-05\n",
      "Epoch 2/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0068 - lr: 5.0000e-05\n",
      "Epoch 3/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0058 - lr: 5.0000e-05\n",
      "Epoch 4/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0053 - lr: 5.0000e-05\n",
      "Epoch 5/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0050 - lr: 5.0000e-05\n",
      "Epoch 6/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0048 - lr: 5.0000e-05\n",
      "Epoch 7/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0046 - lr: 5.0000e-05\n",
      "Epoch 8/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0045 - lr: 5.0000e-05\n",
      "Epoch 9/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0045 - lr: 5.0000e-05\n",
      "Epoch 10/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0043 - lr: 5.0000e-05\n",
      "Epoch 11/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0042 - lr: 5.0000e-05\n",
      "Epoch 12/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0041 - lr: 5.0000e-05\n",
      "Epoch 13/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0041 - lr: 5.0000e-05\n",
      "Epoch 14/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0040 - lr: 5.0000e-05\n",
      "Epoch 15/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0039 - lr: 5.0000e-05\n",
      "Epoch 16/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0039 - lr: 5.0000e-05\n",
      "Epoch 17/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0038 - lr: 5.0000e-05\n",
      "Epoch 18/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0038 - lr: 5.0000e-05\n",
      "Epoch 19/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0037 - lr: 5.0000e-05\n",
      "Epoch 20/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0037 - lr: 5.0000e-05\n",
      "Epoch 21/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0036 - lr: 5.0000e-05\n",
      "Epoch 22/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 23/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0035 - lr: 5.0000e-05\n",
      "Epoch 24/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 25/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 26/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0034 - lr: 5.0000e-05\n",
      "Epoch 27/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 28/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 29/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 30/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 31/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0031 - lr: 5.0000e-05\n",
      "Epoch 32/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 33/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 34/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 35/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0029 - lr: 5.0000e-05\n",
      "Epoch 36/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0029 - lr: 5.0000e-05\n",
      "Epoch 37/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0028 - lr: 5.0000e-05\n",
      "Epoch 38/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0028 - lr: 5.0000e-05\n",
      "Epoch 39/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0027 - lr: 5.0000e-05\n",
      "Epoch 40/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0027 - lr: 5.0000e-05\n",
      "Epoch 41/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0027 - lr: 5.0000e-05\n",
      "Epoch 42/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0026 - lr: 5.0000e-05\n",
      "Epoch 43/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0026 - lr: 5.0000e-05\n",
      "Epoch 44/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0025 - lr: 5.0000e-05\n",
      "Epoch 45/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0025 - lr: 5.0000e-05\n",
      "Epoch 46/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0025 - lr: 5.0000e-05\n",
      "Epoch 47/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 48/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 49/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 50/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 51/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 52/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 53/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0022 - lr: 5.0000e-05\n",
      "Epoch 54/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0022 - lr: 5.0000e-05\n",
      "Epoch 55/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0022 - lr: 5.0000e-05\n",
      "Epoch 56/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0021 - lr: 5.0000e-05\n",
      "Epoch 57/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0021 - lr: 5.0000e-05\n",
      "Epoch 58/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0021 - lr: 5.0000e-05\n",
      "Epoch 59/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0021 - lr: 5.0000e-05\n",
      "Epoch 60/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0021 - lr: 5.0000e-05\n",
      "Epoch 61/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0020 - lr: 5.0000e-05\n",
      "Epoch 62/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0020 - lr: 5.0000e-05\n",
      "Epoch 63/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0020 - lr: 5.0000e-05\n",
      "Epoch 64/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0020 - lr: 5.0000e-05\n",
      "Epoch 65/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0020 - lr: 5.0000e-05\n",
      "Epoch 66/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0019 - lr: 5.0000e-05\n",
      "Epoch 67/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0019 - lr: 5.0000e-05\n",
      "Epoch 68/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0019 - lr: 5.0000e-05\n",
      "Epoch 69/120\n",
      "314/314 [==============================] - 10s 32ms/step - loss: 0.0019 - lr: 5.0000e-05\n",
      "Epoch 70/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0019 - lr: 5.0000e-05\n",
      "Epoch 71/120\n",
      "313/314 [============================>.] - ETA: 0s - loss: 0.0018\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0018 - lr: 5.0000e-05\n",
      "Epoch 72/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0017 - lr: 1.5000e-05\n",
      "Epoch 73/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0017 - lr: 1.5000e-05\n",
      "Epoch 74/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0017 - lr: 1.5000e-05\n",
      "Epoch 75/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0017 - lr: 1.5000e-05\n",
      "Epoch 76/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0017 - lr: 1.5000e-05\n",
      "Epoch 77/120\n",
      "313/314 [============================>.] - ETA: 0s - loss: 0.0017\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 4.499999886320438e-06.\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0017 - lr: 1.5000e-05\n",
      "Epoch 78/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 4.5000e-06\n",
      "Epoch 79/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 4.5000e-06\n",
      "Epoch 80/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 4.5000e-06\n",
      "Epoch 81/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 4.5000e-06\n",
      "Epoch 82/120\n",
      "313/314 [============================>.] - ETA: 0s - loss: 0.0016\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.3499999113264492e-06.\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 4.5000e-06\n",
      "Epoch 83/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 1.3500e-06\n",
      "Epoch 84/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 1.3500e-06\n",
      "Epoch 85/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 1.3500e-06\n",
      "Epoch 86/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 1.3500e-06\n",
      "Epoch 87/120\n",
      "313/314 [============================>.] - ETA: 0s - loss: 0.0016\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.0499998021914507e-07.\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 1.3500e-06\n",
      "Epoch 88/120\n",
      "314/314 [==============================] - 10s 31ms/step - loss: 0.0016 - lr: 4.0500e-07\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_b,\n",
    "    y_train,\n",
    "    verbose=1,\n",
    "    epochs=120,\n",
    "    batch_size=96,\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback, reduce_lr_callback]\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/144 [..............................] - ETA: 48s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:13:40.091829: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'model/conv_lstm2d_1/while/body/_49/model/conv_lstm2d_1/while/Tanh_1' -> 'model/conv_lstm2d_1/while/body/_49/model/conv_lstm2d_1/while/mul_5', 'model/conv_lstm2d_1/while/body/_49/model/conv_lstm2d_1/while/mul_2' -> 'model/conv_lstm2d_1/while/body/_49/model/conv_lstm2d_1/while/add_5', 'model/conv_lstm2d_1/while/body/_49/model/conv_lstm2d_1/while/convolution_7' -> 'model/conv_lstm2d_1/while/body/_49/model/conv_lstm2d_1/while/add_6'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict([X_test_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4608, 12)\n",
      "(4608, 12)\n",
      "--------------------------------------------------------------------------------------\n",
      "mse: 0.0038\n",
      "rmse: 0.0617\n",
      "mae: 0.0417\n",
      "r2: 0.8436\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_final = y_pred.sum(axis=1)/2\n",
    "y_test_final = y_test.sum(axis=1)/2\n",
    "print(y_test_final.shape)\n",
    "print(y_pred_final.shape)\n",
    "mse = mean_squared_error(y_test_final, y_pred_final)\n",
    "rmse = math.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_final, y_pred_final)\n",
    "r2 = r2_score(y_test_final, y_pred_final)\n",
    "print(\"-\" * 86)\n",
    "print(f'mse: {mse:.4f}')\n",
    "print(f'rmse: {rmse:.4f}')\n",
    "print(f'mae: {mae:.4f}')\n",
    "print(f'r2: {r2:.4f}')\n",
    "print(\"-\" * 86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PLOT_DIR = \"./test_plots/residual_hybrid_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TEST_PLOT_DIR):\n",
    "    os.makedirs(TEST_PLOT_DIR)\n",
    "if not os.path.exists(\"./model\"):\n",
    "    os.makedirs(\"./model\")\n",
    "if not os.path.exists(\"./training_history\"):\n",
    "    os.makedirs(\"./training_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = scaler.inverse_transform(y_pred_final)\n",
    "actual_data = scaler.inverse_transform(y_test_final)\n",
    "previous_data = scaler.inverse_transform(X_test_b_flatten)\n",
    "for i in range(actual_data.shape[0]):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    X1 = np.concatenate((X_test_b_timestamp[i][-30:], y_test_timestamp[i]))\n",
    "    y1 = np.concatenate((previous_data[i][-30:], actual_data[i]))\n",
    "    X2 = y_test_timestamp[i]\n",
    "    y_p = pred_data[i]\n",
    "    y_a = actual_data[i]\n",
    "    Xh = np.full(100, X1[len(X1)-12])\n",
    "    yh = np.arange(0, 100, 1)\n",
    "    plt.title(f\"Time Series {i+1} prediction\")\n",
    "    plt.plot(X1, y1, '--', color='#98afc7')\n",
    "    plt.plot(X2, y_p, label='Predict')\n",
    "    plt.plot(X2, y_a, label='Actual')\n",
    "    plt.scatter(X2, y_p)\n",
    "    plt.scatter(X2, y_a)\n",
    "    plt.plot(Xh, yh, color='#4863a0', alpha=0.5)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Usage (kWh)')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(TEST_PLOT_DIR+f\"Time_Series_{i+1}_3days.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model/image_inpainting_CNN_LSTM_3days.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

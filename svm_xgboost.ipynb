{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import xgboost as xgb\n",
    "import os\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs\n",
    "DATA_DIR = \"./load.csv\"\n",
    "TEST_PLOT_DIR = \"./test_plots/svm_xgboost/\"\n",
    "TRAINING_HISTORY_DIR = \"./training_history/svm_xgboost.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "PREDICT_STEP = 96\n",
    "INPUT_STEP = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TEST_PLOT_DIR):\n",
    "    os.makedirs(TEST_PLOT_DIR)\n",
    "if not os.path.exists(\"./model\"):\n",
    "    os.makedirs(\"./model\")\n",
    "if not os.path.exists(\"./training_history\"):\n",
    "    os.makedirs(\"./training_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(df, train_frac=0.70, test_frac=0.25):\n",
    "    # Sort data by year_month\n",
    "    grouped = df.groupby('year_month')\n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "    val_data = pd.DataFrame()\n",
    "\n",
    "    for name, group in grouped:\n",
    "        n = len(group)\n",
    "        train_end = int(train_frac * n)\n",
    "        test_end = train_end + int(test_frac * n)   \n",
    "        train_data = pd.concat([train_data, group.iloc[:train_end]], ignore_index=True)\n",
    "        val_data = pd.concat([val_data, group.iloc[train_end:test_end]], ignore_index=True)\n",
    "        test_data = pd.concat([test_data, group.iloc[test_end:]], ignore_index=True)\n",
    "    # adding time_idx\n",
    "    train_data['time_idx'] = np.arange(len(train_data))\n",
    "    test_data['time_idx'] = np.arange(len(test_data))\n",
    "    val_data['time_idx'] = np.arange(len(val_data))\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back, look_forward):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-look_forward+1):\n",
    "        X = dataset[i:(i+look_back), :]\n",
    "        y = dataset[(i+look_back):(i+look_back+look_forward), 0]\n",
    "        dataX.append(X)\n",
    "        dataY.append(y)\n",
    "    return np.array(dataX).reshape(np.array(dataX).shape[0], np.array(dataX).shape[1]), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_DIR)\n",
    "# Convert the 'date' column to datetime format\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Sort the data by date\n",
    "data.sort_values('Timestamp', inplace=True)\n",
    "\n",
    "# Extract month and year from the date for splitting\n",
    "data['year_month'] = data['Timestamp'].dt.to_period('M')\n",
    "\n",
    "# Splitting the data\n",
    "train_df, test_df, val_df = split_data(data)\n",
    "\n",
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.array(data[\"Load\"]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Load</th>\n",
       "      <th>year_month</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:15:00</td>\n",
       "      <td>10.44</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:30:00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:45:00</td>\n",
       "      <td>10.29</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>10.34</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24517</th>\n",
       "      <td>2023-12-22 15:30:00</td>\n",
       "      <td>6.57</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>24517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24518</th>\n",
       "      <td>2023-12-22 15:45:00</td>\n",
       "      <td>8.22</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>24518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24519</th>\n",
       "      <td>2023-12-22 16:00:00</td>\n",
       "      <td>7.25</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>24519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24520</th>\n",
       "      <td>2023-12-22 16:15:00</td>\n",
       "      <td>7.05</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>24520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24521</th>\n",
       "      <td>2023-12-22 16:30:00</td>\n",
       "      <td>8.30</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>24521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24522 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp   Load year_month  time_idx\n",
       "0     2023-01-01 00:00:00  10.89    2023-01         0\n",
       "1     2023-01-01 00:15:00  10.44    2023-01         1\n",
       "2     2023-01-01 00:30:00  10.89    2023-01         2\n",
       "3     2023-01-01 00:45:00  10.29    2023-01         3\n",
       "4     2023-01-01 01:00:00  10.34    2023-01         4\n",
       "...                   ...    ...        ...       ...\n",
       "24517 2023-12-22 15:30:00   6.57    2023-12     24517\n",
       "24518 2023-12-22 15:45:00   8.22    2023-12     24518\n",
       "24519 2023-12-22 16:00:00   7.25    2023-12     24519\n",
       "24520 2023-12-22 16:15:00   7.05    2023-12     24520\n",
       "24521 2023-12-22 16:30:00   8.30    2023-12     24521\n",
       "\n",
       "[24522 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset(scaler.transform(np.array(train_df['Load']).reshape(-1, 1)), INPUT_STEP, PREDICT_STEP)\n",
    "X_val, y_val = create_dataset(scaler.transform(np.array(val_df['Load']).reshape(-1, 1)), INPUT_STEP, PREDICT_STEP)\n",
    "X_test, y_test = create_dataset(scaler.transform(np.array(test_df['Load']).reshape(-1, 1)), INPUT_STEP, PREDICT_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "X_train: (24139, 288)\n",
      "y_train: (24139, 96)\n",
      "X_val: (8377, 288)\n",
      "y_val: (8377, 96)\n",
      "X_test: (1375, 288)\n",
      "y_test: (1375, 96)\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 86)\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "print(\"-\" * 86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "multioutput_svr = MultiOutputRegressor(svr)\n",
    "multioutput_svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = multioutput_svr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual\n",
    "res = y_val - y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',  # Specify the regression objective\n",
    "    n_estimators=2000,               # Number of boosting rounds (trees)\n",
    "    learning_rate=0.001,              # Step size shrinkage to prevent overfitting\n",
    "    max_depth=7,                    # Maximum depth of a tree\n",
    "    colsample_bytree=0.8,           # Fraction of features used by each tree\n",
    "    subsample=0.8,                  # Fraction of samples used for fitting each tree\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = multioutput_svr.predict(X_test)\n",
    "residual = regressor.predict(prediction)\n",
    "\n",
    "output = prediction + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mean_squared_error(y_test, output)\n",
    "print(\"-\" * 86)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(\"-\" * 86)\n",
    "\n",
    "X_test_reshaped = X_test[:, :, 0]\n",
    "y_test_reshaped = np.reshape(y_test, (y_test.shape[0], y_test.shape[1]))\n",
    "\n",
    "pred_data = np.concatenate(\n",
    "    [scaler.inverse_transform(X_test_reshaped),\n",
    "        scaler.inverse_transform(output)],\n",
    "    axis=-1\n",
    ")\n",
    "actual_data = np.concatenate(\n",
    "    [scaler.inverse_transform(X_test_reshaped),\n",
    "        scaler.inverse_transform(y_test_reshaped)],\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "for i in range(actual_data.shape[0]):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    X = np.arange(1, actual_data.shape[1]+1, 1)\n",
    "    y_pred = pred_data[i]\n",
    "    y_actual = actual_data[i]\n",
    "    plt.title(f\"Time Series {i+1} prediction result\")\n",
    "    plt.plot(X, y_pred, label='Predict')\n",
    "    plt.plot(X, y_actual, label='Actual')\n",
    "    plt.ylim(0, 30)\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Usage (kWh)')\n",
    "    plt.legend()\n",
    "    plt.savefig(TEST_PLOT_DIR+f\"Time_Series_{i+1}.png\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
